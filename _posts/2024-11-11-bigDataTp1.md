---
layout: post
title: TP1 Mise en place d'un traitement en lots 
subtitle: Premier TP
tags: [Big Data, Hadoop, Java, YARN, MapReduce]
author: Mariem ZAOUALI
---

# TP1 : Mise en place d'un traitement en lots : Hadoop, MapReduce et YARN

>**Objectifs du TP :**
> Au terme de ce TP, vous serez capable de :
> - Configurer un cluster avec un Name Node et des Data Nodes
> - Effectuer un programme de Map Reduce sur un fichier avec Hadoop

## Manipulation 1

On va travailler sur la machine virtuelle Ubuntu sur Azure. L'accès vous sera accordé par votre enseignant.
![image](https://github.com/user-attachments/assets/a71d5a4f-3505-4aed-b94f-06c81f4c4439)


Pour démarrer votre environnement Hadoop, lancez la commande suivante à partir de votre terminal:
```cmd
  start-all.sh
```
Pour vérifier que votre environnement Hadoop a été bien lancé, vous devez voir dans la liste des processus : DataNode, NameNode, RessourceManager, NodeManager, et ce en lançant la commande suivante:
```cmd
 jps
```
![image](https://github.com/user-attachments/assets/6d2b4a6e-26ab-4bc9-85e7-fae8344264e4)


On va travailler avec le fichier purchases .txt ( à télécharger [github du cours Udacity]([URL](https://github.com/juandecarrion/udacity-hadoop-course/blob/master/testdata/purchases.txt))). Pour ce faire exécutez la commande suivante pour créer un répertoire sous le nom de input :
```cmd
  hadoop fs -mkdir -p input
```
Ensuite, déplacez le fichier **purchases.txt** vers le répertoire input.
```cmd
  hadoop fs –put purchases.txt input
```
Vérifiez l'existence de ce fichier dans votre HDFS:
```cmd
  hadoop fs –ls input
```
Lancez la commande tail suivante pour avoir l’affichage des dernières lignes du fichier purchases.txt :
```cmd
  hadoop fs -tail input/purchases.txt
```

## Manipulation 2

Dans cette manipulation, on va réaliser un job Map-Reduce. Un Job Map-Reduce se compose principalement de deux types de programmes : 
- **Mappers** : permettent d’extraire les données nécessaires sous forme de clef/valeur, pour pouvoir ensuite les trier selon la clef
-	**Reducers** : prennent un ensemble de données triées selon leur clef, et effectuent le traitement nécessaire sur ces données (somme, moyenne, total...)

Nous testerons un exemple de MapReduce qui va compter les mots dans le fichier purchases.txt. Afin de simplifier ce test de démarrage, je vous invite à cloner une repository de mon compte Github à l’aide de Git en suivant les étapes suivantes :

|L'instruction    | L'illustration      |
|-----------------|---------------------|
| Ouvrir Intellij Idea et cliquez sur « Get from VCS »  | ![image](https://github.com/user-attachments/assets/1a884dfe-a21a-4d7c-b3c9-059f68afcc5c) |
| Rendez-vous à https://github.com/MariemZaouali/enit_tp1_wordcount Et récupérez le lien pour cloner le projet | ![image](https://github.com/user-attachments/assets/897accce-df43-49b2-9a65-f1c4414a218a)|
| Copier le lien et appuyer sur clone. |![image](https://github.com/user-attachments/assets/dd437abd-b9d0-4855-ac2c-1fabdffbf164)|
|Ouvrir le fichier main WordCount et appuyer sur « Current File » puis « Run Configuration ». Choisissez un jdk8 et mettre le chemin vers votre classe main (rectangle jaune), mettez dans le champs Program Arguments : src/main/resources/input/file.txt src/main/resources/output|    ![image](https://github.com/user-attachments/assets/dcabfca3-bbf1-4430-86bf-a1d8bf97ce78)|
| Le fichier à analyser se trouve sous le dossier ressources/input/file.txt. On va tester notre code sur le local sur un fichier léger avant d’aller lancer un job sur le cluster.|![image](https://github.com/user-attachments/assets/bee5c1c5-7366-4f7d-aa8e-e4b53a16e300)|
|Si votre code est encore en rouge, c’est normal nous devons télécharger les dépendances (librairies requises) du projet. |![image](https://github.com/user-attachments/assets/87586b89-9539-48aa-9694-a99103c53ae0)|
|Appuyer sur le menu maven à droite puis sur le bouton de refresh pour lancer le téléchargement des dépendances, puis sur « install ». Le résultat est de vous générer le fichier .jar |![image](https://github.com/user-attachments/assets/3b571985-3e1f-4192-800d-014968af32af)|
|Tout va bien maintenant, allez au main et exécutez ! A la fin de l'exécution un fichier **jar** sera créé.|![image](https://github.com/user-attachments/assets/59880520-7714-490a-a8d2-b6360614201a)|

Un répertoire **output** sera créé dans le répertoire resources, contenant les fichiers résultants de l'exécution du programme **wordcount**. Ouvrez ces fichiers. Vous pouvez changer le contenu du fichier `file.txt` pour voir l'effet du programme Wordcount!

Vous avez deux modes pour passer à l'exécution sur votre machine virtuelle sur le cloud ou sur le cluster.
### Exécution sur la machine virtuelle du cloud
Rendez-vous à l'emplacement de ce jar qui ce trouve dans le répertoire target de votre projet. Une fois bien positionné sur le répertoire en question, vous lancez la commande suivante:

```cmd
hadoop jar Wordcount2-0.jar tn.enit.tp1.WordCount input output
```
Après avoir terminé, visualisez le résultat :
```cmd
hadoop fs -tail output/part-r-00000
```
Vous pouvez accéder à l’interface du namenode via http://localhost:8088.
![image](https://github.com/user-attachments/assets/fe9aa994-f685-4644-8d4a-34a5f903ac17)

Il est également possible de voir le comportement des noeuds esclaves, en allant à l'adresse: http://localhost:8041 pour slave1, et http://localhost:8042 pour slave2. 

### Exécution avec le réseau de docker-compose
En accédant à n'importe quel conteneur (container) vous êtes capables de lancer les commandes de **hdfs** et avoir le même résultat. Mais avant de lancer la commande de **hdfs**, nous allons charger le fichier `purchases.txt` sur un des containers puis nous allons le mettre sur **hdfs**. Donc, la première étape consiste à se placer sous le répertoire où se trouve le fichier `purchases.txt`:
```cmd
sudo docker cp purchases.txt namenode:/purchases.txt
```
Maintenant, nous chargeons ce fichier, qui se trouve maintenant dans le container **namenode**, sur **hdfs**:
```cmd
sudo docker exec -it namenode hdfs dfs -mkdir -p input
```
ensuite:
```cmd
sudo docker exec -it namenode hdfs dfs -put purchases.txt input/purchases.txt
```
Ensuite, nous chargeons le fichier jar `Wordcount2-0.jar`, généré par votre application, sur le même container en se plaçant sous le dossier **target** qui se trouve en local (hors des containers, sur votre machine) et ce en tapant:
```cmd
sudo docker cp Wordcount2-0.jar namenode:/Wordcount2-0.jar
```
Enfin, lancez MapReduce en tapant:

```cmd
sudo docker exec -it namenode hadoop jar Wordcount2-0.jar tn.enit.tp1.WordCount input output
```

Après avoir terminé, visualisez le résultat :
```cmd
sudo docker exec -it namenode hadoop fs -tail output/part-r-00000
```

## Manipulation 3
On va travailler sur d’autres données autre que `purchases.txt`. Rendez-vous au site http://archive.ics.uci.edu et chercher la dataset « census income ». 
1.	Ecrire un programme mapReduce qui vous permet d’afficher la moyenne d’heures de travail par état civil (marital status) vu en cours. Utiliser le combiner dans votre traitement (pensez à utiliser NumPair).
2.	Proposer un traitement mapReduce qui explique si cette population est instruite (choisissez les champs et l’opération qui convient)
3.	[Projet, travail d'équipe] Proposer un traitement MapReduce sur des données d'un domaine de votre choix.



## Manipulation 4 (Optionnel)

Il est demandé de passer d'une architecture namenode + datanode à une architecture contenant namenode + 2 datanodes . Vous devez donc passer par la configuration du fichier `hdfs-site.xml` ainsi que la création des espaces pour chaque datanode. Donner les étapes nécessaires pour le faire.
