---
layout: post
title: TP4 Speed Processing avec Spark Streaming, Data Ingestion using Kafka and Persisting using Cassandra
subtitle: Quatrième TP
tags: [Big Data, Spark, Streaming, Kafka, NoSQL, Cassandra]
author: Mariem ZAOUALI
---
# TP4 : Traitement en temps réel avec Spark Streaming, ingestion des données avec Kafka et persistance avec Cassandra
> **Objectifs du TP :**
>Au terme de ce TP, vous seriez capable de :
>-  Manipuler les structures de Spark à savoir RDD et Dataframes
>-  Tester Spark en mode cluster sans YARN
>-  Implémenter la pipeline HDFS, Spark, Hive et Sqoop pour effectuer un batch processing

## Spark Streaming : le traitement en temps réel

**Spark Streaming** est une extension de l'API principale de **Spark**. Elle permet de créer des applications de streaming évolutives, à haut débit et tolérantes aux pannes, tout cela sans avoir à utiliser un ensemble d'outils complètement différent et en restant dans l'environnement de **Spark**. 
<p align="center">
  <img src="https://github.com/user-attachments/assets/588e5a9c-3f24-48d8-b1b2-34db4d1c78fa" alt="Définition en EN"/>
</p>

Si vous considérez Spark comme une plateforme pour travailler avec **des données par lots**, alors Spark Streaming étend ce concept pour travailler avec des données en unités beaucoup plus petites, mais d'une manière très similaire. Nous appelons cela le **micro-batching**.

<p align="center">
  <img src="https://github.com/user-attachments/assets/f0f27349-9ac3-4e35-bd3f-88a53ed66e4d" alt="Définition en EN"/>
</p>


Spark Streaming peut fonctionner avec une liste impressionnante de sources de données de streaming préconfigurées, Kafka étant probablement la plus importante. Nous parlerons de Kafka plus en détail dans le module suivant. Vous remarquerez également qu'AKKA est présenté différemment. C'est parce qu'il s'agit de la forme la plus brute d'un fournisseur de flux. Spark fournit les interfaces et les connexions nécessaires pour que vous puissiez créer votre propre fournisseur de streaming en utilisant AKKA comme plateforme de messagerie. Spark utilise AKKA pour son implémentation, il est donc naturel de fournir une telle fonctionnalité.

Quoi qu'il en soit, Spark propose une longue liste de sources préconfigurées en plus de celles fournies par la communauté, que vous pouvez adopter, et offre des moyens d'agir sur les données reçues. Par exemple, vous pouvez sauvegarder les résultats dans HDFS ou dans des sources de données externes telles qu'Oracle, SQL Server, Elasticsearch, Cassandra, et bien d'autres.

Au cœur de Spark Streaming se trouve une classe appelée **DStream**. Un DStream n'est qu'une collection de **RDD** avec des informations temporelles. Il est également accompagné de fonctions supplémentaires, par exemple, pour maintenir l'état à mesure que votre application de streaming progresse et pour permettre des calculs de fenêtrage.

Prenons un moment pour comprendre comment fonctionne Spark Streaming. Nous avons évidemment besoin d'un flux de données en entrée qui alimente une application Spark Streaming. Il y a évidemment un processus qui permet à Spark Streaming d'accéder à ce jeu de données en entrée, mais nous n'allons pas nous concentrer là-dessus pour l'instant. Retenez simplement qu'il y a un processus représenté ici par une ligne pointillée verticale dans la diapositive, et ce processus est responsable de fournir les données. Il existe également un composant appelé **block manager** dans Spark, qui garde une trace de ces données lorsqu'elles arrivent dans l'univers Spark.

Dans chaque application de streaming, l'application doit définir un **intervalle de lot**. Il s'agit essentiellement de l'intervalle de micro-batch dont nous parlions plus tôt. Plus l'intervalle est court, plus la latence entre la disponibilité des données en entrée et le début de leur traitement est faible. Cependant, cela ne définit pas nécessairement le temps nécessaire pour terminer le traitement. Idéalement, le traitement se termine dans chaque intervalle de lot. L'intervalle agit simplement comme un déclencheur pour lancer les mêmes étapes de traitement sur le prochain lot de données. Cependant, si le traitement prend plus de temps que l'intervalle de lot, alors le traitement s'accumule. Si cela perdure, l'application devient instable et finit par tomber en panne.

Le point ici est qu'il y a plus d'implications à choisir un intervalle de lot que simplement contrôler la latence. Votre traitement doit être terminé dans ce délai. Le meilleur moyen de vérifier cela est de tester et de résoudre les goulots d'étranglement dans votre code en consultant l'interface utilisateur de Spark Streaming.

Lorsque vos données arrivent, elles sont placées en **blocs** ou en partitions. Un paramètre de configuration de Spark, appelé **spark.streaming.blockInterval**, contrôle l'intervalle auquel les données reçues par les récepteurs de Spark Streaming sont fragmentées en blocs ou partitions. Ces partitions sont liées au temps, et non à la taille. La collection de partitions dans un intervalle de temps similaire basé sur l'intervalle de lot crée collectivement un RDD. Une fois qu'un RDD est créé à partir d'un lot, il est mis en file d'attente et programmé pour exécution par le driver, qui, tout comme dans une application Spark normale, planifie l'exécution de votre code, des transformations et des actions contre les exécuteurs Spark.

## Kafka: Distributed Stream Process

## Manipulation 1: Manipulation kafka data production et Spark
Vous allez mettre votre fichier docker-compose.yml à jour en ajoutant les conteneurs kafka-iot et zookeeper-iot
```yaml
zookeeper:
      image: confluentinc/cp-zookeeper:5.1.0
      hostname: zookeeper
      container_name: zookeeper-iot
      ports:
        - 2181:2181
      networks:
        - net
      environment:
        ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
      image: confluentinc/cp-kafka:5.1.0
      ports:
        - 9092:9092
        - 29092:29092
      depends_on:
        - zookeeper
      environment:
        KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
        KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.request.logger=WARN"
        KAFKA_LOG4J_ROOT_LOGLEVEL: WARN
      volumes:
        - /var/run/docker.sock:/var/run/docker.sock
      hostname: kafka
      container_name: kafka-iot
      networks:
        - net
      restart: always
```
Placez-vous là vous avez votre docker-compose.xml et lancez :
```cmd
docker compose up -d
```
Pour supprimer le réseau, lancez la commande
```cmd
docker compose down
```
Nous allons maintenant tester une logique de Producteur d’information qui est dans notre cas « Kafka »  et un Consommateur qui est dans notre cas « KafkaWordCount.jar » lancé sur spark.
Si jamais, vous n’avez pas installer maven sur le terminal, lancez :
```cmd
apt-get install maven
```
Pour ce faire nous allons créer notre projet java mavenisé :
```cmd
mvn archetype:generate -DgroupId=tn.enit.tp4 -DartifactId=kafka_wordcount -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false
```

Naviguez vers l’emplacement du fichier pom.xml, et mettre ce contenu :
```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>tn.enit.tp4</groupId>
    <artifactId>kafka_wordcount</artifactId>
    <version>1</version>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.2.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.11</artifactId>
            <version>2.2.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-8_2.11</artifactId>
            <version>2.2.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>0.8.2.0</version>
        </dependency>
    </dependencies>

    <build>
        <sourceDirectory>src/main/java</sourceDirectory>
        <testSourceDirectory>src/test/java</testSourceDirectory>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>
            <!--
                         Bind the maven-assembly-plugin to the package phase
              this will create a jar file without the storm dependencies
              suitable for deployment to a cluster.
             -->
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <archive>
                        <manifest>
                            <mainClass>tn.enit.tp4.KafkaWordCount</mainClass>
                        </manifest>
                    </archive>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
            </plugin>
        </plugins>
    </build>

</project>
````

Maintenant, placez-vous sous /src/main/java/tn/enit/tp4, effacez le fichier existant et remplacez-le par KafkaWordCount.java
```java
package tn.enit.tp4;

import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.*;
import org.apache.spark.streaming.kafka.KafkaUtils;

import scala.Tuple2;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;
import java.util.regex.Pattern;

public class KafkaWordCount {
    private static final Pattern SPACE = Pattern.compile(" ");

    private KafkaWordCount() {
    }

    public static void main(String[] args) throws Exception {
        if (args.length < 4) {
            System.err.println("Usage: SparkKafkaWordCount <zkQuorum> <group> <topics> <numThreads>");
            System.exit(1);
        }

        SparkConf sparkConf = new SparkConf().setAppName("KafkaWordCount");
        // Creer le contexte avec une taille de batch de 2 secondes
        JavaStreamingContext jssc = new JavaStreamingContext(sparkConf,
            new Duration(2000));

        int numThreads = Integer.parseInt(args[3]);
        Map<String, Integer> topicMap = new HashMap<>();
        String[] topics = args[2].split(",");
        for (String topic: topics) {
            topicMap.put(topic, numThreads);
        }

        JavaPairReceiverInputDStream<String, String> messages =
                KafkaUtils.createStream(jssc, args[0], args[1], topicMap);

        JavaDStream<String> lines = messages.map(Tuple2::_2);

        JavaDStream<String> words =
                lines.flatMap(x -> Arrays.asList(SPACE.split(x)).iterator());

        JavaPairDStream<String, Integer> wordCounts =
                words.mapToPair(s -> new Tuple2<>(s, 1))
                     .reduceByKey((i1, i2) -> i1 + i2);

        wordCounts.print();
        jssc.start();
        jssc.awaitTermination();
    }
}
```

Lancez maintenant la commande suivante tout en veillant à revenir là où il y a le fichier pom.xml.
```cmd
mvn clean compile assembly:single
```
Cette commande vous générera le fichier executable jar que nous le copions sous le conteneur spark-master :
```cmd
docker cp target/kafka_wordcount-1-jar-with-dependencies.jar spark-master:/
```

Maintenant, connectez-vous à spark-master
```cmd
docker exec -it spark-master bash
```
Et lancez:
```cmd
/spark/bin/spark-submit --class tn.enit.tp4.KafkaWordCount --master local[2] kafka_wordcount-1-jar-with-dependencies.jar zookeeper:2181 test Hello-Kafka 1 >> out
```
Laisser ce console et allez vous connecter au conteneur kafka-iot avec
```cmd
docker exec -it kafka-iot bash
```
Placez-vous sous  /usr/bin
Lancez maintenant :
```cmd
kafka-console-producer --broker-list localhost:9092 --topic Hello-Kafka
```
Envoyez votre message en l'écrivant sur le container de kafka.
Maintenant, quittez kafka container et connectez-vous au container spark-master puis lancez 
```cmd
cat out
``` 

pour voir le résultat du traitement du message envoyé par kafka comme indiqué dans la figure.

![image](https://github.com/user-attachments/assets/67f8e7cb-3b0c-423a-a8e2-e4c1d8d4baab)


## Manipulation 2 : Speed Layer : Spark Streaming, kafka et cassandra
Nous allons créer 3 projets pour mettre à niveau la couche Speed Layer de notre architecture Lambda. Chaque projet sera lancé depuis un conteneur. 
Commençons tout d'abord par le Streaming Job assuré par Spark Streaming. Créer un projet **spark-processor**.
### Traitement avec Spark Streaming
Copiez le fichier `pom.xml`:
```xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>tn.enit.tp4.spark</groupId>
    <artifactId>spark-processor</artifactId>
    <version>1.0.0</version>
    <name>Spark processor for speed layer</name>

    <properties>
        <spark-version>3.1.0</spark-version>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
    </properties>


    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>${spark-version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.12</artifactId>
            <version>${spark-version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.12</artifactId>
            <version>${spark-version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
            <version>${spark-version}</version>
        </dependency>

        <!-- Spark cassandra -->
        <dependency>
            <groupId>com.datastax.spark</groupId>
            <artifactId>spark-cassandra-connector_2.12</artifactId>
            <version>3.0.0</version>
        </dependency>
        <dependency>
            <groupId>joda-time</groupId>
            <artifactId>joda-time</artifactId>
            <version>2.10.9</version>
        </dependency>

        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
            <version>1.2.17</version>
        </dependency>

    </dependencies>
    <build>
        <resources>
            <resource>
                <directory>${basedir}/src/main/resources</directory>
            </resource>
        </resources>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.1</version>
                <configuration>
                    <source>11</source>
                    <target>11</target>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>2.4.3</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <filters>
                                <filter>
                                    <artifact>*:*</artifact>
                                    <excludes>
                                        <exclude>META-INF/*.SF</exclude>
                                        <exclude>META-INF/*.DSA</exclude>
                                        <exclude>META-INF/*.RSA</exclude>
                                    </excludes>
                                </filter>
                            </filters>
                            <transformers>
                                <transformer
                                        implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer">
                                    <resource>reference.conf</resource>
                                </transformer>
                                <transformer
                                        implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <mainClass>tn.enit.tp4.processor.StreamingProcessor</mainClass>
                                </transformer>
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
```
![image](https://github.com/user-attachments/assets/2181a156-f3c2-43f5-b473-623fd36302d7)

Créez sous le package `tn.enit.tp4`, les packages suivants:
-  entity
-  processor
-  util

Dans le package `entity`, ajoutez le fichier `AverageData.java`:
```java
import java.io.Serializable;

public class AverageData implements Serializable {

	private String id;
	private double temperature;

	private double humidity;

	public AverageData() {

	}

	public AverageData(String id, double temperature, double humidity) {
		super();
		this.id = id;
		this.temperature = temperature;
		this.humidity = humidity;
	}

	public String getId() {
		return id;
	}

	public double getTemperature() {
		return temperature;
	}

	public double getHumidity() {
		return humidity;
	}
}
```
Dans le package `entity`, ajoutez le fichier `Humidity.java`:
```java
import java.io.Serializable;
import java.util.Date;

import com.fasterxml.jackson.annotation.JsonFormat;

public class Humidity implements Serializable {

	private String id;
	private double value;
	@JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyy-MM-dd HH:mm:ss", timezone = "MST")
	private Date timestamp;

	public Humidity() {

	}

	public Humidity(String id, double value, Date timestamp) {
		super();
		this.id = id;
		this.value = value;
		this.timestamp = timestamp;
	}

	public String getId() {
		return id;
	}

	public double getValue() {
		return value;
	}

	public Date getTimestamp() {
		return timestamp;
	}

}
```

Dans le package `entity`, ajoutez le fichier `SensorData.java`:
```java
import java.io.Serializable;
import java.util.Date;

import com.fasterxml.jackson.annotation.JsonFormat;

public class SensorData implements Serializable {

	private String id;
	private double temperature;
	private double humidity;
	@JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyy-MM-dd HH:mm:ss", timezone = "MST")
	private Date timestamp;

	public SensorData() {

	}

	public SensorData(String id, double temperature, double humidity, Date timestamp) {
		super();
		this.id = id;
		this.temperature = temperature;
		this.humidity = humidity;
		this.timestamp = timestamp;
	}

	public String getId() {
		return id;
	}

	public double getTemperature() {
		return temperature;
	}

	public double getHumidity() {
		return humidity;
	}

	public Date getTimestamp() {
		return timestamp;
	}

}
```
Dans le package `entity`, ajoutez le fichier `Temperature.java`:
```java
import java.io.Serializable;
import java.util.Date;

import com.fasterxml.jackson.annotation.JsonFormat;

public class Temperature implements Serializable{
	
	private String id;
	private double value;
	@JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyy-MM-dd HH:mm:ss", timezone="MST")
	private Date timestamp;
	
	public Temperature(){
		
	}

	public Temperature(String id, double value, Date timestamp) {
		super();
		this.id = id;
		this.value = value;
		this.timestamp = timestamp;
	}

	public String getId() {
		return id;
	}

	public double getValue() {
		return value;
	}

	public Date getTimestamp() {
		return timestamp;
	}

}
```
Dans le package `processor`, ajoutez le fichier `ProcessorUtils.java`:
```java
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SaveMode;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.streaming.api.java.JavaDStream;

import tn.enit.tp4.entity.AverageData;
import tn.enit.tp4.entity.Humidity;
import tn.enit.tp4.entity.SensorData;
import tn.enit.tp4.entity.Temperature;
import com.datastax.spark.connector.japi.CassandraJavaUtil;

import static com.datastax.spark.connector.japi.CassandraStreamingJavaUtil.javaFunctions;

import java.sql.Date;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Properties;

class ProcessorUtils {

	public static SparkConf getSparkConf(Properties prop) {
		var sparkConf = new SparkConf().setAppName(prop.getProperty("com.iot.app.spark.app.name"))
				.setMaster(prop.getProperty("com.iot.app.spark.master"))
				.set("spark.cassandra.connection.host", prop.getProperty("com.iot.app.cassandra.host"))
				.set("spark.cassandra.connection.port", prop.getProperty("com.iot.app.cassandra.port"))
				.set("spark.cassandra.auth.username", prop.getProperty("com.iot.app.cassandra.username"))
				.set("spark.cassandra.auth.password", prop.getProperty("com.iot.app.cassandra.password"))
				.set("spark.cassandra.connection.keep_alive_ms", prop.getProperty("com.iot.app.cassandra.keep_alive"));

		if ("local".equals(prop.getProperty("com.iot.app.env"))) {
			sparkConf.set("spark.driver.bindAddress", "127.0.0.1");
		}
		return sparkConf;
	}

	public static void saveTemperatureToCassandra(final JavaDStream<Temperature> dataStream) {
		System.out.println("Saving to cassandra...");

		// Map Cassandra table column
		HashMap<String, String> columnNameMappings = new HashMap<>();
		columnNameMappings.put("id", "id");
		columnNameMappings.put("timestamp", "timestamp");
		columnNameMappings.put("value", "value");

		// call CassandraStreamingJavaUtil function to save in DB
		javaFunctions(dataStream).writerBuilder("sensordatakeyspace", "temperature",
				CassandraJavaUtil.mapToRow(Temperature.class, columnNameMappings)).saveToCassandra();
	}

	public static void saveHumidityToCassandra(final JavaDStream<Humidity> dataStream) {
		System.out.println("Saving to cassandra...");

		// Map Cassandra table column
		HashMap<String, String> columnNameMappings = new HashMap<>();
		columnNameMappings.put("id", "id");
		columnNameMappings.put("timestamp", "timestamp");
		columnNameMappings.put("value", "value");

		// call CassandraStreamingJavaUtil function to save in DB
		javaFunctions(dataStream).writerBuilder("sensordatakeyspace", "humidity",
				CassandraJavaUtil.mapToRow(Humidity.class, columnNameMappings)).saveToCassandra();
	}

	public static void saveAvgToCassandra(JavaRDD<AverageData> rdd) {
		CassandraJavaUtil.javaFunctions(rdd)
				.writerBuilder("sensordatakeyspace", "averagedata", CassandraJavaUtil.mapToRow(AverageData.class))
				.saveToCassandra();

	}

	public static void saveDataToHDFS(final JavaDStream<SensorData> dataStream, String saveFile, SparkSession sql) {
		System.out.println("Saving to hdfs...");

		dataStream.foreachRDD(rdd -> {
			if (rdd.isEmpty()) {
				return;
			}
			Dataset<Row> dataFrame = sql.createDataFrame(rdd, SensorData.class);

			Dataset<Row> dfStore = dataFrame.selectExpr("id", "temperature", "humidity", "timestamp");
			dfStore.printSchema();
			dfStore.write().mode(SaveMode.Append).parquet(saveFile);
		});
	}

	public static SensorData transformData(Row row) {
		System.out.println(row);
		return new SensorData(row.getString(0), row.getDouble(1), row.getDouble(2), new Date(2022, 5, 5));
	}

	public static List<AverageData> runBatch(SparkSession sparkSession, String saveFile) {
		System.out.println("Running Batch Processing");
		var dataFrame = sparkSession.read().parquet(saveFile);
		System.out.println(dataFrame);
		JavaRDD<SensorData> rdd = dataFrame.javaRDD().map(row -> ProcessorUtils.transformData(row));

		JavaRDD<Double> temp = rdd.map(data -> {
			return data.getTemperature();
		});

		JavaRDD<Double> hum = rdd.map(data -> {
			return data.getHumidity();
		});

		double avg_temp = temp.reduce((value1, value2) -> value1 + value2);

		double avg_hum = hum.reduce((value1, value2) -> value1 + value2);

		long length = temp.count();

		avg_temp /= length;

		avg_hum /= length;

		System.out.println("Avg temp : " + avg_temp);

		System.out.println("Avg hum : " + avg_hum);

		AverageData d = new AverageData("0", avg_temp, avg_hum);
		List<AverageData> average_data_list = new ArrayList<AverageData>();
		average_data_list.add(d);

		return average_data_list;
	}

}
```
Dans le package `processor`, ajoutez le fichier `StreamProcessor.java`:
```java
import java.util.*;
import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.*;
import org.apache.spark.streaming.kafka010.*;

import tn.enit.tp4.entity.AverageData;
import tn.enit.tp4.entity.Humidity;
import tn.enit.tp4.entity.SensorData;
import tn.enit.tp4.entity.Temperature;
import tn.enit.tp4.util.SensorDataDeserializer;

import tn.enit.tp4.util.PropertyFileReader;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;

public class StreamProcessor {

	public static void main(String[] args) throws Exception {

		String file = "spark-processor.properties";
		Properties prop = PropertyFileReader.readPropertyFile(file);

		SparkConf conf = ProcessorUtils.getSparkConf(prop);

		JavaStreamingContext streamingContext = new JavaStreamingContext(conf, Durations.seconds(10));
		JavaSparkContext sc = streamingContext.sparkContext();

		streamingContext.checkpoint(prop.getProperty("com.iot.app.spark.checkpoint.dir"));

		Map<String, Object> kafkaParams = new HashMap<>();
		kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, prop.getProperty("com.iot.app.kafka.brokerlist"));
		kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
		kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, SensorDataDeserializer.class);
		kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, prop.getProperty("com.iot.app.kafka.topic"));
		kafkaParams.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, prop.getProperty("com.iot.app.kafka.resetType"));
		kafkaParams.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);

		Collection<String> topics = Arrays.asList(prop.getProperty("com.iot.app.kafka.topic"));

		JavaInputDStream<ConsumerRecord<String, SensorData>> stream = KafkaUtils.createDirectStream(streamingContext,
				LocationStrategies.PreferConsistent(),
				ConsumerStrategies.<String, SensorData>Subscribe(topics, kafkaParams));

		JavaDStream<SensorData> sensordataStream = stream.map(v -> {
			return v.value();
		});

		sensordataStream.print();

		JavaDStream<Temperature> temperatureStream = sensordataStream.map(v -> {
			return new Temperature(v.getId(), v.getTemperature(), v.getTimestamp());
		});
		temperatureStream.print();

		JavaDStream<Humidity> humidityStream = sensordataStream.map(v -> {
			return new Humidity(v.getId(), v.getHumidity(), v.getTimestamp());
		});

		// save data to cassandra => stream
		ProcessorUtils.saveTemperatureToCassandra(temperatureStream);

		ProcessorUtils.saveHumidityToCassandra(humidityStream);
		
		

		// batch process
		// save data to HDFS => batch
		SparkSession sparkSession = SparkSession.builder().config(conf).getOrCreate();
		String saveFile = prop.getProperty("com.iot.app.hdfs") + "iot-data";
		ProcessorUtils.saveDataToHDFS(sensordataStream, saveFile, sparkSession);

		// sparkSession.close();
		// sparkSession.stop();

		streamingContext.start();
		streamingContext.awaitTermination();
	}
}
```
Dans le package `util`, créez le fichier `PropertyFileReader.java`:
```java
import java.io.IOException;
import java.io.InputStream;
import java.util.Properties;

import org.apache.log4j.Logger;

/**
 * Utility class to read property file
 */
public class PropertyFileReader {

    private static final Logger logger = Logger.getLogger(PropertyFileReader.class);

    private static Properties prop = new Properties();

    public static Properties readPropertyFile(String file) throws Exception {
        if (prop.isEmpty()) {
            InputStream input = PropertyFileReader.class.getClassLoader().getResourceAsStream(file);
            try {
                prop.load(input);
            } catch (IOException ex) {
                logger.error(ex);
                throw ex;
            } finally {
                if (input != null) {
                    input.close();
                }
            }
        }
        return prop;
    }
}
```
Dans le package `util`, créez le fichier `SensorDataDeserializer.java`:
```java
import tn.enit.tp4.entity.SensorData;
import com.fasterxml.jackson.databind.ObjectMapper;

import org.apache.kafka.common.serialization.Deserializer;

import java.util.Map;


public class SensorDataDeserializer implements Deserializer<SensorData> {
	
	private static ObjectMapper objectMapper = new ObjectMapper();

	public SensorData fromBytes(byte[] bytes) {
		try {
			return objectMapper.readValue(bytes, SensorData.class);
		} catch (Exception e) {
			e.printStackTrace();
		}
		return null;
	}

	@Override
	public void configure(Map<String, ?> map, boolean b) {

	}

	@Override
	public SensorData deserialize(String s, byte[] bytes) {
		return fromBytes((byte[]) bytes);
	}

	@Override
	public void close() {

	}
}
```
Dans le dossier `ressources`, vous ajoutez les fichiers suivants: `spark-processor-local.properties`
```cmd
#Kafka  properties
com.iot.app.kafka.zookeeper=localhost:2181
com.iot.app.kafka.brokerlist=localhost:29092
com.iot.app.kafka.topic=sensor-data
com.iot.app.kafka.resetType=earliest

#Spark properties
com.iot.app.spark.app.name=Iot Data Processor
com.iot.app.spark.master=local[*]
com.iot.app.spark.checkpoint.dir=/tmp/iot-streaming-data
com.iot.app.hdfs=/Users/LENOVO/eclipse-workspace/BigData/spark-processor/data/
com.iot.app.jar=/Users/LENOVO/eclipse-workspace/BigData/spark-processor/target/spark-processor-1.0.0.jar

#Cassandra propertis
com.iot.app.cassandra.host=127.0.0.1
com.iot.app.cassandra.port=9042
com.iot.app.cassandra.keep_alive=10000
com.iot.app.cassandra.username=cassandra
com.iot.app.cassandra.password=cassandra

# Miscellaneous
com.iot.app.env=local
```
et le fichier  `spark-processor.properties`
```yml
# Kafka  properties
com.iot.app.kafka.zookeeper=zookeeper:2181
com.iot.app.kafka.brokerlist=kafka:9092
com.iot.app.kafka.topic=sensor-data
com.iot.app.kafka.resetType=earliest

#Spark properties
com.iot.app.spark.app.name=Iot Data Processor
com.iot.app.spark.master=spark://spark-master:7077
com.iot.app.spark.checkpoint.dir=hdfs://namenode:8020/lambda-arch/checkpoint
com.iot.app.hdfs=hdfs://namenode:8020/lambda-arch/
com.iot.app.jar=/opt/spark-data/bigdata-spark-processor-1.0.0.jar

#Cassandra propertis
com.iot.app.cassandra.host=172.22.0.6
com.iot.app.cassandra.port=9042
com.iot.app.cassandra.keep_alive=10000
com.iot.app.cassandra.username=cassandra
com.iot.app.cassandra.password=cassandra

# Miscellaneous
com.iot.app.env=cluster
```
L'arborescence de votre projet ressemblera finalement à celà:
![image](https://github.com/user-attachments/assets/20c5619f-24c3-4cc6-94c6-c06402884876)

### Production des données avec Kafka
Créez maintenant un projet `kafka-producer`. Dans le projet `pom.xml`:
```xml
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>tn.enit.tp4.kafka</groupId>
    <artifactId>kafka-producer</artifactId>
    <version>1.0.0</version>
    <name>Kafka Producer</name>

    <dependencies>

        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka_2.10</artifactId>
            <version>0.9.0.0</version>
        </dependency>


        <!-- Jackson dependencies for JSON handling -->
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-core</artifactId>
            <version>2.15.2</version>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
            <version>2.15.2</version>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-annotations</artifactId>
            <version>2.15.2</version>
        </dependency>

        <!-- Logging -->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>2.0.9</version>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
            <version>2.0.9</version>
        </dependency>

        <!-- Testing -->
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter-api</artifactId>
            <version>5.10.0</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter-engine</artifactId>
            <version>5.10.0</version>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <resources>
            <resource>
                <directory>${basedir}/src/main/resources</directory>
            </resource>
        </resources>
        <plugins>
            <!-- Compiler plugin for Java 11+ -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.11.0</version>
                <configuration>
                    <source>11</source>
                    <target>11</target>
                </configuration>
            </plugin>

            <!-- Maven Shade Plugin for creating a fat JAR -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.5.0</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <transformers>
                                <transformer
                                        implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <mainClass>tn.enit.tp4.kafka.SensorDataProducer</mainClass>
                                </transformer>
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
```
Nous créeons le package `tn.enit.tp4.kafka`.
Nous allons créer `PropertyFileReader.java`:
```java
package tn.enit.tp4.kafka;

import java.io.IOException;
import java.io.InputStream;
import java.util.Properties;

public class PropertyFileReader {
    private static Properties prop = new Properties();
    public static Properties readPropertyFile() throws Exception {
        if (prop.isEmpty()) {
            InputStream input = PropertyFileReader.class.getClassLoader().getResourceAsStream("kafka-producer.properties");
            try {
                prop.load(input);
            } catch (IOException ex) {
                System.out.println(ex.toString());
                throw ex;
            } finally {
                if (input != null) {
                    input.close();
                }
            }
        }
        return prop;
    }
}
```
Créez un fichier `SensorData.java` :
```java
package tn.enit.tp4.kafka;

import java.io.Serializable;
import java.util.Date;

import com.fasterxml.jackson.annotation.JsonFormat;

public class SensorData implements Serializable {

    private String id;
    private double temperature;
    private double humidity;
    @JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyy-MM-dd HH:mm:ss", timezone = "MST")
    private Date timestamp;

    public SensorData() {

    }

    public SensorData(String id, double temperature, double humidity, Date timestamp) {
        super();
        this.id = id;
        this.temperature = temperature;
        this.humidity = humidity;
        this.timestamp = timestamp;
    }

    public String getId() {
        return id;
    }

    public double getTemperature() {
        return temperature;
    }

    public double getHumidity() {
        return humidity;
    }

    public Date getTimestamp() {
        return timestamp;
    }

}
```
Créez le fichier `SensorDataEncoder.java`: 
```java
package tn.enit.tp4.kafka;

import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.kafka.common.serialization.Serializer;

import java.util.Map;

public class SensorDataEncoder implements Serializer<SensorData> {

    private final ObjectMapper objectMapper = new ObjectMapper();

    @Override
    public void configure(Map<String, ?> configs, boolean isKey) {
        // No special configuration required
    }

    @Override
    public byte[] serialize(String topic, SensorData data) {
        try {
            return objectMapper.writeValueAsBytes(data);
        } catch (Exception e) {
            throw new RuntimeException("Failed to serialize SensorData", e);
        }
    }

    @Override
    public void close() {
        // No resources to close
    }
}
```
Créez le fichier `SensorDataProducer.java`:
```java
package tn.enit.tp4.kafka;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.ProducerConfig;

import java.util.Date;
import java.util.Properties;
import java.util.Random;
import java.util.UUID;

public class SensorDataProducer {

    private final KafkaProducer<String, SensorData> producer;

    public SensorDataProducer(KafkaProducer<String, SensorData> producer) {
        this.producer = producer;
    }

    public static void main(String[] args) throws Exception {
        // Configure the producer properties
        Properties properties = new Properties();
        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "tn.enit.tp4.kafka.SensorDataEncoder");

        // Create a Kafka producer
        KafkaProducer<String, SensorData> producer = new KafkaProducer<>(properties);

        // Instantiate the SensorDataProducer and generate events
        SensorDataProducer sensorProducer = new SensorDataProducer(producer);
        sensorProducer.generateIoTEvent("sensor-data-topic");
    }

    private void generateIoTEvent(String topic) throws InterruptedException {
        Random rand = new Random();
        double initTemp = 20;
        double initHum = 80;
        System.out.println("Sending events...");

        while (true) {
            SensorData event = generateSensorData(rand, initTemp, initHum);
            producer.send(new ProducerRecord<>(topic, event.getId(), event));
            Thread.sleep(rand.nextInt(3000) + 2000); // Random delay between 2 and 5 seconds
        }
    }

    private SensorData generateSensorData(final Random rand, double temp, double hum) {
        String id = UUID.randomUUID().toString();
        Date timestamp = new Date();
        double temperature = temp + rand.nextDouble() * 10;
        double humidity = hum + rand.nextDouble() * 10;

        return new SensorData(id, temperature, humidity, timestamp);
    }
}
```
Dans le dossier ressources, ajoutez le fichier `kafka-producer.properties`: 
```yml
# Kafka  properties
#### if running from the host the kafka port will be 29092, when running from docker it will be 9092
zookeeper.connect=localhost:2181
metadata.broker.list=localhost:29092
request.required.acks=1
serializer.class=tn.enit.tp4.kafka.SensorDataEncoder
kafka.topic=sensor-data

```
L'arborescence de votre projet doit ressembler à celà à la fin :
![image](https://github.com/user-attachments/assets/7d8cc508-5cd0-45f3-91b7-8be28ec4654c)

### Configurer le docker-compose
Maintenant, mettez tous les projets dans un seul dossier de cette manière (un nouveau dossier `data`sera créé aussi):
![image](https://github.com/user-attachments/assets/6620eb8f-cd37-4ebf-b3fb-118615d97dd4)

Créez votre fichier `docker-compose.yml`:
```yml
version: "3.3"


networks:
  netw:
    driver: bridge
    ipam:
      config:
        - subnet: 172.23.0.0/24

services:

  zookeeper:
      image: confluentinc/cp-zookeeper:5.1.0
      hostname: zookeeper
      container_name: zookeeper-iot
      ports:
        - 2181:2181
      networks:
        - netw
      environment:
        ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
      image: confluentinc/cp-kafka:5.1.0
      ports:
        - 9092:9092
        - 29092:29092
      depends_on:
        - zookeeper
      environment:
        KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
        KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.request.logger=WARN"
        KAFKA_LOG4J_ROOT_LOGLEVEL: WARN
      volumes:
        - /var/run/docker.sock:/var/run/docker.sock
      hostname: kafka
      container_name: kafka-iot
      networks:
        - netw
      restart: always

  cassandra:
    image: 'bitnami/cassandra:latest'
    hostname: cassandra
    networks:
      netw:
        ipv4_address: 172.23.0.6
    ports:
      - "9042:9042"
    environment:
      - "MAX_HEAP_SIZE=256M"
      - "HEAP_NEWSIZE=128M"
    container_name: cassandra-iot
    volumes:
      - ./data/schema.cql:/schema.cql

  spark-master:
    image: bde2020/spark-master:3.0.0-hadoop3.2-java11
    container_name: spark-master
    hostname: spark-master
    healthcheck:
      interval: 5s
      retries: 100
    ports:
      - "8080:8080"
      - "7077:7077"
      - "4040:4040"
      - "4041:4041"
    environment:
      - INIT_DAEMON_STEP=false
      - SPARK_DRIVER_HOST=192.168.1.5
    volumes:
      - ./spark-processor/target:/opt/spark-data
    networks:
      - netw
  spark-worker-1:
    image: bde2020/spark-worker:3.0.0-hadoop3.2-java11
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    volumes:
      - ./data/spark/:/opt/spark-data
    networks:
      - netw
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.1.3-java8
    container_name: namenode
    hostname: namenode
    volumes:
    - ./data/namenode:/hadoop/dfs/name
    environment:
    - CLUSTER_NAME=test
    - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    healthcheck:
      interval: 5s
      retries: 100
    networks:
    - netw
    ports:
    - 9870:9870
    - 8020:8020
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.1.3-java8
    container_name: datanode
    hostname: datanode
    volumes:
    - ./data/datanode:/hadoop/dfs/data
    environment:
    - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - namenode
    healthcheck:
      interval: 5s
      retries: 100
    networks:
     - netw
    ports:
     - 50075:50075
     - 50010:50010
```

Créez un dossier `data` et créez le fichier `schema.cql`:
```cql
//Create keyspace
CREATE KEYSPACE IF NOT EXISTS sensordatakeyspace WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};

//Create table
CREATE TABLE sensordatakeyspace.temperature (id text , timeStamp timestamp, value double, PRIMARY KEY (id));

CREATE TABLE sensordatakeyspace.humidity (id text , timeStamp timestamp, value double, PRIMARY KEY (id));

CREATE TABLE sensordatakeyspace.averagedata (id text , temperature double, humidity double, PRIMARY KEY (id));
```
Pour exécuter votre speed layer sur containers, lancez:
```cmd
docker-compose up -d
```
Vous devez aussi lancer la création de schema sous cassandra:
```cmd
docker exec cassandra-iot cqlsh --username cassandra --password cassandra -f /schema.cql
```
Lancez le "Kafka Data production":
```cmd
docker exec kafka kafka-console-producer.sh --broker-list localhost:9092 --topic sensor-data
```

Lancez le `Stream Processor`:
```cmd
docker exec spark-master /spark/bin/spark-submit --class tn.enit.tp4.StreamProcessor --master spark://localhost:7077 /opt/spark-data/<notre.jar>
```

## A vous de jouer
Reportez le traitement effectué au niveau de la manipulation 2 sur vos propores données.
