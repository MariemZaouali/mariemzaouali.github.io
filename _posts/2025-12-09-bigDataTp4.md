---
layout: post
title: TP4 Putting them all together l'architecture Lambda
subtitle: Quatrième TP
tags: [Big Data, Spark, Streaming, Kafka, NoSQL, Cassandra]
author: Mariem ZAOUALI
---
# TP4 : Putting them all together : l'architecture Lambda
> **Objectifs du TP :**
>Au terme de ce TP, vous seriez capable de pouvoir implémenter un projet mettant en oeuvre l'architecture Lambda
>-  Batch processing Layer
>-  Speed processing Layer
>-  Serving Layer

# Manipulation 1 : Speed Layer : Spark Streaming, kafka et cassandra

Nous allons créer 3 projets pour mettre à niveau la couche Speed Layer de notre architecture Lambda. Chaque projet sera lancé depuis un conteneur. 
Commençons tout d'abord par le Streaming Job assuré par Spark Streaming. 
Pour ce faire, mettez d'abord votre `docker-compose.yml` à jour en ajoutant le conteneur kafka.
```yaml
 spark-master:
    image: bitnami/spark:3.5.1
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    ports:
      - "7077:7077"
      - "8080:8080"
    networks:
      - net

  spark-worker-1:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
    ports:
      - "8081:8081"
    depends_on:
      - spark-master
    networks:
      - net

  spark-worker-2:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
    ports:
      - "8082:8081"
    depends_on:
      - spark-master
    networks:
      - net
  kafka:
    image: apache/kafka:3.7.1
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:19093"
      KAFKA_LISTENERS: "PLAINTEXT://kafka:9092,CONTROLLER://kafka:19093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://localhost:9092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LOG_DIRS: /tmp/kraft-combined-logs
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    networks:
      - net
    volumes:
      - kafka_data:/tmp/kraft-combined-logs

networks:
  net:
  cassandra:
    image: cassandra:5.0
    container_name: cassandra
    ports:
      - "9042:9042"
      - "9160:9160"
    environment:
      CASSANDRA_CLUSTER_NAME: "BigDataCluster"
      CASSANDRA_NUM_TOKENS: 8
      CASSANDRA_START_RPC: "true"
      CASSANDRA_SEEDS: "cassandra"
      CASSANDRA_ENABLE_USER_DEFINED_FUNCTIONS: "true"
      MAX_HEAP_SIZE: 2G
      HEAP_NEWSIZE: 1G
    volumes:
      - cassandra_data:/var/lib/cassandra
    networks:
      - net
    restart: always
```
Créer un projet maintenant s'appelant **spark-processor** ayant cette arborescence:
```yaml
spark-processor/
│
├── pom.xml                          # Fichier Maven avec toutes les dépendances (Spark 3.5.1, Kafka 3.7.0, Cassandra 5.0)
├── README.md
│
├── src/
│   └── main/
│       ├── java/
│       │   └── tn/
│       │       └── enit/
│       │           └── tp4/
│       │               ├── processor/
│       │               │   ├── StreamProcessor.java        # Classe principale Structured Streaming
│       │               │   └── ProcessorUtils.java        # Utils pour Spark, HDFS et Cassandra
│       │               │
│       │               ├── entity/
│       │               │   ├── SensorData.java
│       │               │   ├── Temperature.java
│       │               │   ├── Humidity.java
│       │               │   └── AverageData.java
│       │               │
│       │               └── util/
│       │                   ├── PropertyFileReader.java
│       │                   └── SensorDataDeserializer.java
│       │
│       └── resources/
│           ├── spark-processor.properties                # Propriétés locales ou cluster
│           └── log4j2.xml                                 # Configuration Log4j 2
│
└── target/                                                # Généré par Maven (jar final)
    └── spark-processor-1.0.0.jar
```
Pour ce projet, nous allons utiliser les versions suivantes :
-  Spark	3.5.1	Compatible Hadoop 3.x & Kafka 3.x
-  Kafka	3.7.1	Compatible Spark Structured Streaming
-  Cassandra 5.0
  
### Traitement avec Spark Streaming

Copiez le fichier `pom.xml`:
```xml
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>tn.enit.tp4.spark</groupId>
    <artifactId>spark-processor</artifactId>
    <version>1.0.0</version>
    <name>Spark processor for speed layer</name>

    <properties>
        <spark.version>3.5.1</spark.version>
        <scala.binary.version>2.12</scala.binary.version>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
    </properties>

    <dependencies>

        <!-- Spark Core -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <!-- Spark SQL (obligatoire pour Structured Streaming) -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <!-- Structured Streaming + Kafka -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql-kafka-0-10_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <!-- Kafka Clients latest -->
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>3.7.0</version>
        </dependency>

        <!-- Cassandra Connector compatible Spark 3.5.x -->
        <dependency>
            <groupId>com.datastax.spark</groupId>
            <artifactId>spark-cassandra-connector_${scala.binary.version}</artifactId>
            <version>3.5.0</version>
        </dependency>

        <!-- Time utilities -->
        <dependency>
            <groupId>joda-time</groupId>
            <artifactId>joda-time</artifactId>
            <version>2.12.7</version>
        </dependency>

        <!-- log4j secure -->
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-core</artifactId>
            <version>2.23.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-api</artifactId>
            <version>2.23.1</version>
        </dependency>

    </dependencies>

    <build>
        <resources>
            <resource>
                <directory>${basedir}/src/main/resources</directory>
            </resource>
        </resources>

        <plugins>

            <!-- JAVA 11 -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.11.0</version>
                <configuration>
                    <source>11</source>
                    <target>11</target>
                </configuration>
            </plugin>

            <!-- UBER JAR -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.5.1</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals><goal>shade</goal></goals>
                        <configuration>
                            <filters>
                                <filter>
                                    <artifact>*:*</artifact>
                                    <excludes>
                                        <exclude>META-INF/*.SF</exclude>
                                        <exclude>META-INF/*.DSA</exclude>
                                        <exclude>META-INF/*.RSA</exclude>
                                    </excludes>
                                </filter>
                            </filters>
                            <transformers>
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <mainClass>tn.enit.tp4.processor.StreamingProcessor</mainClass>
                                </transformer>
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin>

        </plugins>
    </build>

</project>

```

Créez sous le package `tn.enit.tp4`, les packages suivants:
-  entity
-  processor
-  util

Dans le package `entity`, ajoutez le fichier `AverageData.java`:
```java
import java.io.Serializable;

public class AverageData implements Serializable {

	private String id;
	private double temperature;

	private double humidity;

	public AverageData() {

	}

	public AverageData(String id, double temperature, double humidity) {
		super();
		this.id = id;
		this.temperature = temperature;
		this.humidity = humidity;
	}

	public String getId() {
		return id;
	}

	public double getTemperature() {
		return temperature;
	}

	public double getHumidity() {
		return humidity;
	}
}
```
Dans le package `entity`, ajoutez le fichier `Humidity.java`:
```java
import java.io.Serializable;
import java.util.Date;

import com.fasterxml.jackson.annotation.JsonFormat;

public class Humidity implements Serializable {

	private String id;
	private double value;
	@JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyy-MM-dd HH:mm:ss", timezone = "MST")
	private Date timestamp;

	public Humidity() {

	}

	public Humidity(String id, double value, Date timestamp) {
		super();
		this.id = id;
		this.value = value;
		this.timestamp = timestamp;
	}

	public String getId() {
		return id;
	}

	public double getValue() {
		return value;
	}

	public Date getTimestamp() {
		return timestamp;
	}

}
```

Dans le package `entity`, ajoutez le fichier `SensorData.java`:
```java
import java.io.Serializable;
import java.util.Date;

import com.fasterxml.jackson.annotation.JsonFormat;

public class SensorData implements Serializable {

	private String id;
	private double temperature;
	private double humidity;
	@JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyy-MM-dd HH:mm:ss", timezone = "MST")
	private Date timestamp;

	public SensorData() {

	}

	public SensorData(String id, double temperature, double humidity, Date timestamp) {
		super();
		this.id = id;
		this.temperature = temperature;
		this.humidity = humidity;
		this.timestamp = timestamp;
	}

	public String getId() {
		return id;
	}

	public double getTemperature() {
		return temperature;
	}

	public double getHumidity() {
		return humidity;
	}

	public Date getTimestamp() {
		return timestamp;
	}

}
```
Dans le package `entity`, ajoutez le fichier `Temperature.java`:
```java
import java.io.Serializable;
import java.util.Date;

import com.fasterxml.jackson.annotation.JsonFormat;

public class Temperature implements Serializable{
	
	private String id;
	private double value;
	@JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyy-MM-dd HH:mm:ss", timezone="MST")
	private Date timestamp;
	
	public Temperature(){
		
	}

	public Temperature(String id, double value, Date timestamp) {
		super();
		this.id = id;
		this.value = value;
		this.timestamp = timestamp;
	}

	public String getId() {
		return id;
	}

	public double getValue() {
		return value;
	}

	public Date getTimestamp() {
		return timestamp;
	}

}
```
Dans le package `processor`, ajoutez le fichier `ProcessorUtils.java`:
```java
package tn.enit.tp4.processor;

import org.apache.spark.SparkConf;
import org.apache.spark.sql.*;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;

import tn.enit.tp4.entity.AverageData;
import tn.enit.tp4.entity.SensorData;

import com.datastax.spark.connector.cql.CassandraConnector;

import java.time.LocalDate;
import java.util.List;
import java.util.Properties;

public class ProcessorUtils {

    
    // Spark Configuration (Structured Streaming)
    public static SparkSession createSparkSession(Properties prop) {
        SparkConf conf = new SparkConf()
                .setAppName(prop.getProperty("tn.enit.tp4.spark.app.name"))
                .setMaster(prop.getProperty("tn.enit.tp4.spark.master"))
                .set("spark.cassandra.connection.host", prop.getProperty("tn.enit.tp4.cassandra.host"))
                .set("spark.cassandra.connection.port", prop.getProperty("tn.enit.tp4.cassandra.port"))
                .set("spark.cassandra.auth.username", prop.getProperty("tn.enit.tp4.cassandra.username"))
                .set("spark.cassandra.auth.password", prop.getProperty("tn.enit.tp4.cassandra.password"));

        if ("local".equals(prop.getProperty("tn.enit.tp4.env"))) {
            conf.set("spark.driver.bindAddress", "127.0.0.1");
        }

        return SparkSession.builder()
                .config(conf)
                .getOrCreate();
    }

    
    // Read data from Kafka as Structured Stream
    public static Dataset<Row> readFromKafka(SparkSession spark, String kafkaBootstrap, String topic) {

        return spark
                .readStream()
                .format("kafka")
                .option("kafka.bootstrap.servers", kafkaBootstrap)
                .option("subscribe", topic)
                .option("startingOffsets", "earliest")
                .load()
                .selectExpr(
                        "CAST(key AS STRING)",
                        "CAST(value AS STRING)",
                        "timestamp"
                );
    }

   
    // Parse JSON String to SensorData Dataset
    public static Dataset<SensorData> parseSensorData(Dataset<Row> kafkaDF) {

        // Suppose value column contains JSON like {"id":"...","temperature":..,"humidity":..}
        return kafkaDF.selectExpr("CAST(value AS STRING) AS json_str")
                .as(Encoders.STRING())
                .map(json -> {
                    // Simple parsing (you can use Gson or Jackson)
                    String[] parts = json.replaceAll("[{}\"]", "").split(",");
                    String id = parts[0].split(":")[1];
                    double temperature = Double.parseDouble(parts[1].split(":")[1]);
                    double humidity = Double.parseDouble(parts[2].split(":")[1]);
                    return new SensorData(id, temperature, humidity, java.sql.Date.valueOf(LocalDate.now()));
                }, Encoders.bean(SensorData.class));
    }

    // Write SensorData to Cassandra (Structured Streaming)
    public static void writeToCassandra(Dataset<SensorData> sensorDS, String keyspace, String table) {

        try {
            StreamingQuery query = sensorDS.writeStream()
                    .foreachBatch((batchDF, batchId) -> {
                        batchDF.write()
                                .format("org.apache.spark.sql.cassandra")
                                .option("keyspace", keyspace)
                                .option("table", table)
                                .mode("append")
                                .save();
                    })
                    .outputMode("update")
                    .start();

            query.awaitTermination();

        } catch (StreamingQueryException e) {
            e.printStackTrace();
        }
    }

    // Write SensorData to HDFS
    public static void writeToHDFS(Dataset<SensorData> sensorDS, String savePath) {
        try {
            StreamingQuery query = sensorDS.writeStream()
                    .foreachBatch((batchDF, batchId) -> {
                        batchDF.write()
                                .mode(SaveMode.Append)
                                .parquet(savePath);
                    })
                    .outputMode("append")
                    .start();

            query.awaitTermination();

        } catch (StreamingQueryException e) {
            e.printStackTrace();
        }
    }

    // Compute Average Temperature & Humidity (Batch)
    public static AverageData computeAverage(SparkSession spark, String hdfsPath) {
        Dataset<SensorData> df = spark.read().parquet(hdfsPath).as(Encoders.bean(SensorData.class));

        double avgTemp = df.agg(functions.avg("temperature")).first().getDouble(0);
        double avgHum = df.agg(functions.avg("humidity")).first().getDouble(0);

        return new AverageData("0", avgTemp, avgHum);
    }

}

```
Dans le package `processor`, ajoutez le fichier `StreamProcessor.java`:
```java
package tn.enit.tp4.processor;

import org.apache.spark.sql.*;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;

import tn.enit.tp4.entity.SensorData;
import tn.enit.tp4.entity.Temperature;
import tn.enit.tp4.entity.Humidity;

import tn.enit.tp4.util.PropertyFileReader;

import java.util.Properties;

public class StreamProcessor {

    public static void main(String[] args) throws StreamingQueryException {

        
        // Charger les propriétés
        String file = "spark-processor.properties";
        Properties prop = PropertyFileReader.readPropertyFile(file);

        
        // Créer SparkSession
        SparkSession spark = ProcessorUtils.createSparkSession(prop);

        String kafkaBootstrap = prop.getProperty("tn.enit.tp4.brokerlist");
        String kafkaTopic = prop.getProperty("tn.enit.tp4.topic");
        String cassandraKeyspace = "sensordatakeyspace";

        // Lire depuis Kafka
        Dataset<Row> kafkaRawDF = ProcessorUtils.readFromKafka(spark, kafkaBootstrap, kafkaTopic);

        
        // Convertir en SensorData Dataset
        Dataset<SensorData> sensorDS = ProcessorUtils.parseSensorData(kafkaRawDF);

        
        // Séparer Temperature et Humidity
        Dataset<Temperature> tempDS = sensorDS.map(
                s -> new Temperature(s.getId(), s.getTemperature(), s.getTimestamp()),
                Encoders.bean(Temperature.class)
        );

        Dataset<Humidity> humDS = sensorDS.map(
                s -> new Humidity(s.getId(), s.getHumidity(), s.getTimestamp()),
                Encoders.bean(Humidity.class)
        );

        
        // Écriture vers Cassandra
        ProcessorUtils.writeToCassandra(tempDS, cassandraKeyspace, "temperature");
        ProcessorUtils.writeToCassandra(humDS, cassandraKeyspace, "humidity");

        
        // Écriture vers HDFS
        String hdfsPath = prop.getProperty("tn.enit.tp4.hdfs.path");
        if (hdfsPath != null && !hdfsPath.isEmpty()) {
            ProcessorUtils.writeToHDFS(sensorDS, hdfsPath);
        }

        // Attendre la fin du streaming
        spark.streams().awaitAnyTermination();
    }
}

```
Dans le package `util`, créez le fichier `PropertyFileReader.java`:
```java
package tn.enit.tp4.util;

import java.io.IOException;
import java.io.InputStream;
import java.util.Properties;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 * Utility class to read property files
 */
public class PropertyFileReader {

    private static final Logger logger = LogManager.getLogger(PropertyFileReader.class);

    private static Properties prop = new Properties();

    public static Properties readPropertyFile(String file) throws IOException {
        if (prop.isEmpty()) {
            try (InputStream input = PropertyFileReader.class.getClassLoader().getResourceAsStream(file)) {
                if (input == null) {
                    throw new IOException("Property file '" + file + "' not found in the classpath");
                }
                prop.load(input);
                logger.info("Properties file '{}' loaded successfully", file);
            } catch (IOException ex) {
                logger.error("Error loading property file '{}': {}", file, ex.getMessage());
                throw ex;
            }
        }
        return prop;
    }
}
```
Dans le package `util`, créez le fichier `SensorDataDeserializer.java`:
```java
package tn.enit.tp4.util;

import tn.enit.tp4.entity.SensorData;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.kafka.common.serialization.Deserializer;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.util.Map;

public class SensorDataDeserializer implements Deserializer<SensorData> {

    private static final Logger logger = LogManager.getLogger(SensorDataDeserializer.class);
    private static final ObjectMapper objectMapper = new ObjectMapper();

    @Override
    public void configure(Map<String, ?> configs, boolean isKey) {
        // Pas de configuration spécifique
    }

    @Override
    public SensorData deserialize(String topic, byte[] data) {
        if (data == null) {
            return null;
        }
        try {
            return objectMapper.readValue(data, SensorData.class);
        } catch (Exception e) {
            logger.error("Error deserializing SensorData from topic {}: {}", topic, e.getMessage(), e);
            return null;
        }
    }

    @Override
    public void close() {
        // Pas de ressources à fermer
    }
}

```
Dans le dossier `ressources`, vous ajoutez les fichiers suivants: `spark-processor-local.properties`
```cmd
# -----------------------------
# Kafka properties
# -----------------------------
tn.enit.tp4.brokerlist=localhost:29092
tn.enit.tp4.topic=sensor-data
tn.enit.tp4.resetType=earliest

# -----------------------------
# Spark properties
# -----------------------------
tn.enit.tp4.spark.app.name=Iot Data Processor
tn.enit.tp4.spark.master=local[*]
tn.enit.tp4.spark.checkpoint.dir=/tmp/iot-streaming-data
tn.enit.tp4.hdfs.path=/home/IdeaProjects/spark-processor/data/
tn.enit.tp4.jar.path=/home/IdeaProjects/spark-processor/target/spark-processor-1.0.0.jar

# -----------------------------
# Cassandra properties
# -----------------------------
tn.enit.tp4.cassandra.host=127.0.0.1
tn.enit.tp4.cassandra.port=9042
tn.enit.tp4.cassandra.keep_alive=10000
tn.enit.tp4.cassandra.username=cassandra
tn.enit.tp4.cassandra.password=cassandra
tn.enit.tp4.cassandra.keyspace=sensordatakeyspace

# -----------------------------
# Miscellaneous
# -----------------------------
tn.enit.tp4.env=local
```
et le fichier  `spark-processor.properties`
```yml
# -----------------------------
# Kafka properties
# -----------------------------
tn.enit.tp4.brokerlist=kafka:9092
tn.enit.tp4.topic=sensor-data
tn.enit.tp4.resetType=earliest
# zookeeper n'est plus obligatoire pour les clients Kafka récents
# tn.enit.tp4.kafka.zookeeper=zookeeper:2181

# -----------------------------
# Spark properties
# -----------------------------
tn.enit.tp4.spark.app.name=Iot Data Processor
tn.enit.tp4.spark.master=spark://spark-master:7077
tn.enit.tp4.spark.checkpoint.dir=hdfs://namenode:8020/lambda-arch/checkpoint
tn.enit.tp4.hdfs.path=hdfs://namenode:8020/lambda-arch/
tn.enit.tp4.jar.path=spark-processor-1.0.0.jar

# -----------------------------
# Cassandra properties
# -----------------------------
tn.enit.tp4.cassandra.host=cassandra
tn.enit.tp4.cassandra.port=9042
tn.enit.tp4.cassandra.keep_alive=10000
tn.enit.tp4.cassandra.username=cassandra
tn.enit.tp4.cassandra.password=cassandra
tn.enit.tp4.cassandra.keyspace=sensordatakeyspace

# -----------------------------
# Miscellaneous
# -----------------------------
tn.enit.tp4.env=cluster
```
L'arborescence de votre projet ressemblera finalement à celà:
![image](https://github.com/user-attachments/assets/20c5619f-24c3-4cc6-94c6-c06402884876)

### Production des données avec Kafka
> Attention ⚠️
> Java 8 est installée sur le container du kafka!
> Vous avez besoin de générer un jar avec cette version. Pour ce faire, on doit mettre cette version dans le pom.xml comme l'indique le fichier suivant ⬇️.
>

Créez maintenant un projet `kafka-producer` avec l'arborescence suivante:
```yaml
kafka-producer/
│
├── pom.xml                           # Dépendances Kafka, Jackson, SLF4J, JUnit
├── README.md
│
├── src/
│   └── main/
│       ├── java/
│       │   └── tn/
│       │       └── enit/
│       │           └── tp4/
│       │               └── kafka/
│       │                   ├── PropertyFileReader.java    # Lecture des propriétés Kafka
│       │                   ├── SensorData.java            # Entité SensorData
│       │                   ├── SensorDataEncoder.java     # Sérialiseur Kafka
│       │                   └── SensorDataProducer.java    # Producteur Kafka principal
│       │
│       └── resources/
│           └── kafka-producer.properties                  # Configuration Kafka
│
└── target/                                                 # Jar généré après compilation
    └── kafka-producer-1.0.0.jar
```

Dans le projet, créez `pom.xml`:
```xml
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>tn.enit.tp4.kafka</groupId>
    <artifactId>kafka-producer</artifactId>
    <version>1.0.0</version>
    <name>Kafka Producer</name>

    <properties>
        <kafka.version>3.7.0</kafka.version> <!-- version récente compatible Java 8 -->
        <jackson.version>2.15.2</jackson.version>
        <slf4j.version>2.0.9</slf4j.version>
        <junit.version>5.10.0</junit.version>
    </properties>

    <dependencies>

        <!-- Kafka Clients -->
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>${kafka.version}</version>
        </dependency>

        <!-- Jackson dependencies for JSON handling -->
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-core</artifactId>
            <version>${jackson.version}</version>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
            <version>${jackson.version}</version>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-annotations</artifactId>
            <version>${jackson.version}</version>
        </dependency>

        <!-- Logging -->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>${slf4j.version}</version>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
            <version>${slf4j.version}</version>
        </dependency>

        <!-- Testing -->
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter-api</artifactId>
            <version>${junit.version}</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter-engine</artifactId>
            <version>${junit.version}</version>
            <scope>test</scope>
        </dependency>

    </dependencies>

    <build>
        <resources>
            <resource>
                <directory>${basedir}/src/main/resources</directory>
            </resource>
        </resources>
        <plugins>
            <!-- Compiler plugin for Java 8 -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.11.0</version>
                <configuration>
                    <source>8</source>
                    <target>8</target>
                </configuration>
            </plugin>

            <!-- Maven Shade Plugin for creating a fat JAR -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.5.0</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <transformers>
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <mainClass>tn.enit.tp4.kafka.SensorDataProducer</mainClass>
                                </transformer>
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>

```
Nous créeons le package `tn.enit.tp4.kafka`.
Nous allons créer `PropertyFileReader.java`:
```java
package tn.enit.tp4.kafka;

import java.io.IOException;
import java.io.InputStream;
import java.util.Properties;

public class PropertyFileReader {
    private static Properties prop = new Properties();

    public static Properties readPropertyFile() throws Exception {
        if (prop.isEmpty()) {
            try (InputStream input = PropertyFileReader.class.getClassLoader()
                    .getResourceAsStream("kafka-producer.properties")) {
                
                if (input == null) {
                    throw new IOException("Fichier kafka-producer.properties introuvable dans le classpath");
                }
                
                prop.load(input);
            } catch (IOException ex) {
                System.err.println("Erreur lors de la lecture du fichier de propriétés : " + ex.getMessage());
                throw ex;
            }
        }
        return prop;
    }
}

```
Créez un fichier `SensorData.java` :
```java
package tn.enit.tp4.kafka;

import java.io.Serializable;
import java.util.Date;
import com.fasterxml.jackson.annotation.JsonFormat;

public class SensorData implements Serializable {

    private String id;
    private double temperature;
    private double humidity;

    @JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyy-MM-dd HH:mm:ss", timezone = "MST")
    private Date timestamp;

    public SensorData() { }

    public SensorData(String id, double temperature, double humidity, Date timestamp) {
        this.id = id;
        this.temperature = temperature;
        this.humidity = humidity;
        this.timestamp = timestamp;
    }

    // Getters
    public String getId() { return id; }
    public double getTemperature() { return temperature; }
    public double getHumidity() { return humidity; }
    public Date getTimestamp() { return timestamp; }

    // Setters (nécessaires pour Jackson)
    public void setId(String id) { this.id = id; }
    public void setTemperature(double temperature) { this.temperature = temperature; }
    public void setHumidity(double humidity) { this.humidity = humidity; }
    public void setTimestamp(Date timestamp) { this.timestamp = timestamp; }

    @Override
    public String toString() {
        return "SensorData{" +
                "id='" + id + '\'' +
                ", temperature=" + temperature +
                ", humidity=" + humidity +
                ", timestamp=" + timestamp +
                '}';
    }
}

```
Créez le fichier `SensorDataEncoder.java`: 
```java
package tn.enit.tp4.kafka;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.kafka.common.serialization.Serializer;

import java.util.Map;

public class SensorDataSerializer implements Serializer<SensorData> {

    private static final ObjectMapper objectMapper = new ObjectMapper();

    @Override
    public void configure(Map<String, ?> configs, boolean isKey) {
        // Pas nécessaire pour l'instant
    }

    @Override
    public byte[] serialize(String topic, SensorData data) {
        try {
            String msg = objectMapper.writeValueAsString(data);
            System.out.println("Serialized: " + msg);
            return msg.getBytes();
        } catch (JsonProcessingException e) {
            System.err.println("Error in serialization: " + e.getMessage());
            return null;
        }
    }

    @Override
    public void close() {
        // Pas nécessaire
    }
}

```
Créez le fichier `SensorDataProducer.java`:
```java
package tn.enit.tp4.kafka;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.ProducerConfig;

import java.util.Properties;
import java.util.Random;
import java.util.UUID;
import java.util.Date;

public class SensorDataProducer {

    private final KafkaProducer<String, SensorData> producer;

    public SensorDataProducer(KafkaProducer<String, SensorData> producer) {
        this.producer = producer;
    }

    public static void main(String[] args) throws Exception {
        // Lire les propriétés
        Properties properties = PropertyFileReader.readPropertyFile();

        // Configurer Kafka moderne
        Properties kafkaProps = new Properties();
        kafkaProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, properties.getProperty("metadata.broker.list"));
        kafkaProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        kafkaProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "tn.enit.tp4.kafka.SensorDataSerializer");
        kafkaProps.put(ProducerConfig.ACKS_CONFIG, properties.getProperty("request.required.acks", "1"));

        KafkaProducer<String, SensorData> producer = new KafkaProducer<>(kafkaProps);
        SensorDataProducer iotProducer = new SensorDataProducer(producer);

        iotProducer.generateIoTEvent(properties.getProperty("kafka.topic"));
    }

    private void generateIoTEvent(String topic) throws InterruptedException {
        Random rand = new Random();
        double init_val_temp = 20;
        double init_val_hum = 80;

        System.out.println("Sending events...");

        while (true) {
            SensorData event = generateSensorData(rand, init_val_temp, init_val_hum);
            System.out.println("Sent: " + event.getId() + " Temp:" + event.getTemperature() + " Hum:" + event.getHumidity());
            
            // Envoi avec Kafka moderne
            producer.send(new ProducerRecord<>(topic, event.getId(), event));

            Thread.sleep(rand.nextInt(3000) + 2000); // délai aléatoire de 2 à 5 secondes
        }
    }

    private SensorData generateSensorData(final Random rand, double temp, double hum) {
        String id = UUID.randomUUID().toString();
        Date timestamp = new Date();
        double t = temp + rand.nextDouble() * 10;
        double h = hum + rand.nextDouble() * 10;

        return new SensorData(id, t, h, timestamp);
    }
}


```
Dans le dossier ressources, ajoutez le fichier `kafka-producer.properties`: 
```yml
# Kafka properties (modern Kafka Producer)
# Si Docker: kafka:9092, si host: localhost:29092
bootstrap.servers=localhost:29092
acks=1
key.serializer=org.apache.kafka.common.serialization.StringSerializer
value.serializer=tn.enit.tp4.kafka.SensorDataSerializer
kafka.topic=sensor-data
```

Créez un dossier `data` et créez le fichier `schema.cql`:
```cql
//Create keyspace
CREATE KEYSPACE IF NOT EXISTS sensordatakeyspace WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};

//Create table
CREATE TABLE sensordatakeyspace.temperature (id text , timeStamp timestamp, value double, PRIMARY KEY (id));

CREATE TABLE sensordatakeyspace.humidity (id text , timeStamp timestamp, value double, PRIMARY KEY (id));

CREATE TABLE sensordatakeyspace.averagedata (id text , temperature double, humidity double, PRIMARY KEY (id));
```
Pour exécuter votre speed layer sur containers, lancez:
```cmd
docker-compose up -d
```

> ⚠️ Attenion !
> Si un container est appelé à utiliser un port déjà pris par une autre application, ce port ne peut pas être exploité. Exemple, si vous tentez de monter le container spark-master alos que son port 8081 est occupé, vous devez lancer cette commande pour savoir qui utilise ce port:
> ```cmd
> sudo lsof -i :8081
> ```
> Une fois identifié, vous lancez `sudo kill -9 <PID>`
>

Il faut créer tout d'abord les dossiers suivants sur HDFS. Ces dossiers sont hyperimportants pour l'enregistrement des DAG du Spark Streaming.
```cmd
docker exec namenode hdfs dfs -rm -r /lambda-arch
docker exec namenode hdfs dfs -mkdir -p /lambda-arch
docker exec namenode hdfs dfs -mkdir -p /lambda-arch/checkpoint
docker exec namenode hdfs dfs -chmod -R 777 /lambda-arch
docker exec namenode hdfs dfs -chown -R 777 /lambda-arch
docker exec namenode hdfs dfs -chmod -R 777 /lambda-arch/checkpoint
docker exec namenode hdfs dfs -chown -R 777 /lambda-arch/checkpoint
```
Vous devez aussi lancer la création de schema sous cassandra:
```cmd
docker exec cassandra-iot cqlsh --username cassandra --password cassandra -f /schema.cql
```
Lancez le "Kafka Data production" en lançant depuis docker en chargeant le fichier `jar` sur votre container. Placez-vous sous le dossier target du projet kafka-producer et lancez:
```cmd
docker cp kafka-producer-1.0.0.jar kafka-iot:/
```
Puis:
```cmd
docker exec -it kafka java -jar kafka-producer-1.0.0.jar
```
Vous obtiendrez l'affichage suivant :
![image](https://github.com/user-attachments/assets/0b27689f-3b3c-43c9-b7c3-fc74805b9ca6)

Nous faisons de même pour le container `spark-master`en chargeant le fichier `jar` qui se trouve dans le dossier du projet spark-processer avec la commande:
```cmd
 docker cp spark-processor-1.0.0.jar spark-master:/
```
Lancez le `Stream Processor`:
```cmd
docker exec spark-master /spark/bin/spark-submit --class tn.enit.tp4.processor.StreamProcessor /spark-processor-1.0.0.jar
```
Afin de visualiser les résultats, connectez-vous à cassandra:
```cmd
docker exec -it cassandra-iot cqlsh -u cassandra -p cassandra
```
Puis lancez:
```cmd
DESCRIBE KEYSPACES;
SELECT * FROM sensordatakeyspace.temperature;
SELECT * FROM sensordatakeyspace.humidity;
```
Vous obtiendrez des résultats similaires à ceux-là ⬇️
![image](https://github.com/user-attachments/assets/04c3744e-3f58-408e-bfef-916ec2555783)
