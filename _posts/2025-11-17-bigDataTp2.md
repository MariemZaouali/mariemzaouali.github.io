---
layout: post
title: TP2 Manipulation des données avec Apache Hive
subtitle: Deuxième TP
tags: [Big Data, Hive, Hadoop]
author: Mariem ZAOUALI
---

# TP2 :  Manipulation des données avec Apache Hive

>**Objectifs du TP :**
> Au terme de ce TP, vous serez capable de :
> - Manipuler des commandes HQL (Hive Query Language) qui vous permettront de créer, charger, modifier, supprimer les données
> - Préparer les données avec Hive pour qu’elles soient exploitées par le traitement par lot (batch processing) avec Hadoop MapReduce
> - Appliquer des méthodes d'optimisation comme CBO et PPD



**Hive** est un système d’entrepôt des données (Datawarehouse) qui est utilisé pour le requêtage et l’analyse des larges datasets stockées dans HDFS. Il se base sur MapReduce pour effectuer ses traitements.

Télécharger le fichier `zip` suivant dans un répertoire de votre choix. 
```link
https://drive.google.com/file/d/1VovgEoee92-Yc0KeWt1oMoOumGdzpMai/view?usp=sharing
```
Décompressez le fichier. Ouvrez le terminal et placez-vous sous le dossier où il y a le fichier `docker-compose.yml`. Dans ce fichier,
vous verrez que cetains services reliés à hive ont été ajoutés.

| Service                     | Rôle                                                                        |
|-----------------------------|------------------------------------------------------------------------------|
| `hive-metastore-postgresql` | Base de données relationnelle utilisée pour stocker les métadonnées Hive     |
| `hive-metastore`            | Service Metastore (Thrift, port 9083) qui gère l’accès aux métadonnées Hive |
| `hive-server`               | HiveServer2 (port 10000) pour exécuter les requêtes SQL via Beeline/JDBC    |

Lancez maintenant la commande:

```sql
sudo docker compose up -d
```
Connectez-vous au serveur de hive:
```sql
sudo docker exec -it hive-server bash
```
puis tapez:
```cmd
hive
```
Vous aurez une ligne de commande qui commence par :
```cmd
hive>
```
Si vous voulez sortir vous tapez :
```cmd
quit;
```

Des erreurs rencontrées: 

Pour accéder à Hive cli pour la toute première fois sur votre machine, vous devez lancer le service:
```cmd
hive --service metastore &
```
Si on t'affiche l'erreur de **Namenode is in Safe mode**, lancez cette commande:
```link
hdfs dfsadmin -safemode leave
```
## Row format 
Nous découvrons dans cette section, comment créer une table avec des champs délimités par ‘,’ (Remarque : dans l’affichage de la table, vous ne verrez pas les ‘,’, c’est au niveau du fichier physique que les données seront séparées par ‘,’).
 	
  Faites des captures écran pour garder la trace de votre travail pour la validation !
Si une commande de création de table ou de base ne marche pas car la table/base existe, appelez **DROP DATABASE <bd_name>** ; ou **DROP TABLE <table_name> ;**

Tapez :
```sql
CREATE TABLE jobs 
(id INT, title STRING, salary INT, posted TIMESTAMP) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',';
```
Vous avez créé une table sous la base **default**. Vérifiez l'existence de cette table avec la commande: 
```sql
SHOW TABLES;
```
Remplissez cette table avec:
```sql
INSERT INTO jobs 
VALUES (1,'Data Analyst',135000,'2016-12-21 15:52:03');
```
Remarquez le lancement d'un job MapReduce pour faire cette insertion! Pour une insertion, la latence est forte.
Visualisez cette ligne bien enregistrée dans votre table en tapant:
```sql
SELECT * FROM JOBS;
```
Quittez hive cli  avec quit ; 
```sql
quit;
```
Le répertoire `/user/hive/warehouse` est le répertoire que hive utilise pour stocker ses bases de données et ses tables par défaut.
Visualisez le contenu de ce répertoire:
```sql
hdfs dfs -ls  /user/hive/warehouse
```
Vous devez voir votre table jobs sous forme d'un répertoire. Affichez encore le contenu de ce répertoire:
```sql
hdfs dfs -ls  /user/hive/warehouse/jobs
```
Le fichier `000000_0` contient les lignes de votre table?
Affichez le contenu du fichier à l’aide de la commande
```cmd
hdfs dfs -cat /user/hive/warehouse/jobs/000000_0
```
Notez que les valeurs de chaîne et d'horodatage stockées dans ce fichier ne sont pas placées entre guillemets. Des guillemets ont été utilisés dans l'instruction INSERT pour encadrer la chaîne littérale et les valeurs d'horodatage, mais Hive ne stocke pas ces guillemets dans les fichiers de données de la table.
Reconnectez-vous à hive cli avec la commande hive.
```cmd
hive
```
Exécutez maintenant :
```sql
CREATE TABLE jobs_tsv 
(id INT, title STRING, salary INT, posted TIMESTAMP) 
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
```
Insérez une ligne dans cette table:
```sql
INSERT INTO jobs_tsv 
VALUES (1,'Data Analyst',135000,'2016-12-21 15:52:03');
```
Quittez hive :
```sql
quit;
```
Examinez le contenu du fichier **/user/hive/warehouse/jobs_tsv** pour voir la différence entre les deux délimiteurs ‘,’ et ‘\t’ en visualisant le contenu du fichier sous le répertoire **jobs_tsv**.
```sql
hdfs dfs -cat /user/hive/warehouse/jobs_tsv/000000_0
```
Dans Hive, les délimiteurs , (virgule) et \t (tabulation) sont utilisés pour séparer les colonnes lors de l'importation ou de l'exportation de données dans des fichiers. On préfère l'usage du délimiteur **tabulation** quand les données elles-mêmes contiennent des **virgule** car la tabulation est moins susceptible d'y apparaître. Si vous ne précisez pas le délimiteur, hive éventuellement considérera que vos données sont séparées par des espaces ou des tabulations.

## Table externe
Nous avons vu que hive stocke les tables dans un répertoire par défaut. Nous allons, voir dans cette manipulation, que hive peut stocker des tables en dehors de ce répertoire, mais dans ce cas, il les considère comme des tables externes. La spécificité de celles-ci est qu'elles ne seront pas supprimées du disque en lançant la commande **DROP TABLE**, mais seulement elle seront inacessibles depuis hive. Ces tables sont particulièrement utiles lorsqu'on souhaite conserver les données sur le disque pour d'autres usages ou lorsqu'elles sont partagées avec d'autres systèmes.

Créez une table EXTERNE avec la commande suivante :
```sql
CREATE EXTERNAL TABLE default.investors 
(name STRING, amount INT, share DECIMAL(4,3)) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',';
```
Sans la précision du clause LOCATION, hive va la créer sous `/user/hive/warehouse/`.
Tapez:
```sql
INSERT INTO default.investors (name, amount, share)
VALUES ('Karim Beguir', 10000, 0.325);
```
Vérifiez qu'elle figure parmi les autres tables :
```sql
show tables;
```
Quittez hive en tapant :
```sql
quit;
```
Visualisez le contenu du répertoire de hive et vous verrez que la table existe quand même.
```sql
hdfs dfs -ls /user/hive/warehouse
```
Effacez la table:
```sql
DROP TABLE default.investors;
```
Re-chargez la table avec:
```sql
CREATE EXTERNAL TABLE default.investors(name STRING, amount INT, share DECIMAL(4,3));
```
Observez le résultat:
```sql
SELECT * FROM investors ;
```
Vous allez remarquer que hive supprime le schéma et non pas les données physiques avec la table "EXTERNAL".
## Stored as
Créez une nouvelle table avec :
```sql
CREATE TABLE jobs_txt (id INT, title STRING, salary INT, posted TIMESTAMP) 
STORED AS TEXTFILE;
```
Insérez une ligne dans la table :	
```sql
INSERT INTO jobs_txt VALUES (1,'Data Analyst',135000,'2016-12-21 15:52:03');
```
Examinez le fichier sous le répertoire **/user/hive/warehouse/jobs_txt** . Remarquez que vous pouvez clairement voir le contenu du fichier et ses valeurs dedans.

Maintenant, on va créer un autre format, le format « **parquet** » :
```sql
CREATE TABLE jobs_parquet (id INT, title STRING, salary INT, posted TIMESTAMP) 
STORED AS PARQUET ;
```
Insérez une ligne dans la table :
```sql
INSERT INTO jobs_parquet VALUES (1,'Data Analyst',135000,'2016-12-21 15:52:03');
```
Examinez le fichier sous le répertoire **/user/hive/warehouse/jobs_parquet**. Remarquez que vous ne voyez pas un contenu cohérent mais plutôt il y a du code ASCII dedans !
## Location
Créez la table :
```sql
CREATE TABLE jobs_training 
(id INT, title STRING, salary INT, posted TIMESTAMP) 
LOCATION '/user/training/jobs_training/';
```
Vérifiez la création du répertoire «**/user/training/jobs_training** » et que le dossier est vide jusqu’à présent.
```sql
INSERT INTO jobs_training 
VALUES (1,'Data Analyst',135000,'2016-12-21 15:52:03');
```
Regardez une autre fois dans le répertoire de la table et vérifiez si vous avez bien ajouté cette ligne.
## If not exits and LIKE
Nous pouvons créer une table ayant le même schéma qu'une table existante.
Tapez:
```sql
CREATE TABLE new_investors LIKE investors;
```
Si vous tentez d'exécuter cette même commande une autre fois, vous obtiendrez une erreur! Pour éviter que celà puisse apparaître, vous utilisez IF NOT EXISTS:
```sql
CREATE TABLE IF NOT EXISTS new_investors LIKE investors;
```
Tapez :
```sql
INSERT INTO new_investors SELECT * from investors;
```
et observez le contenu de la table **new_investors**:
```sql
SELECT * FROM new_investors;
```

## Table description
Tapez :
```sql
DESCRIBE jobs_txt ;
DESCRIBE FORMATTED jobs_txt ;
```
Si vous obtenez la valeur MANAGED_TABLE, c’est parce que la table n’a pas été créée en tant que EXTERNAL.
## Table modification
Beaucoup de modifications sont possibles avec ALTER. Modifions une table interne par une table externe :
```sql
ALTER TABLE investors SET TBLPROPERTIES('EXTERNAL'='TRUE');
```
On peut aussi changer son nom:
```sql
ALTER TABLE investors RENAME TO companies ;
```
Également, on peut changer la base de données hôte :
```sql
CREATE DATABASE IF NOT EXISTS section;
ALTER TABLE default.companies RENAME TO section.companies;
```
Vous pouvez également changer les types des champs avec ALTER.
## Partitioning
Dans hive, nous pouvons effectuer deux types de partitionnement des données : statique et dynamique.
Le partitionnement statique dans Hive signifie que les partitions sont explicitement spécifiées par l'utilisateur lors de l'insertion des données dans une table partitionnée. L'utilisateur doit définir les valeurs de partition pour chaque partition dans la requête INSERT. Les partitions sont donc créées au moment de l'insertion.
Par exemple:
```sql
CREATE TABLE sales (
    product STRING,
    amount INT
)
PARTITIONED BY (year INT, month INT);

-- Insertion de données dans une partition spécifique (année 2023, mois 1)
INSERT INTO sales PARTITION (year=2023, month=1)
VALUES ('product1', 100);
```
Quant au partionnement dynamique, il est recommandé pour traiter une grande quantité de données à insérer et où les partitions peuvent être différentes pour chaque lot de données. Ainsi, les partitions seront créées d'une manière automatique. Prenons un exemple:

Quittez hive
```cmd
quit;
```
Téléchargez **customers.csv** à partir du lien:
```link
https://drive.google.com/drive/folders/1yHi5gPFu_4999GhdemwRD8ux2mllufU8?usp=drive_link
```
Tapez:
```cmd
mv Downloads/customers.csv customers.csv
```
Chargerons le fichier **customers.csv** sur HDFS avec la commande :
```sql
hdfs dfs -put customers.csv /user/hive/warehouse/customers.csv
```
Accédez maintenant à hive CLI:
```sql
hive
```
Créez la table customers où on chargera le fichier customers.csv. Remarquez le champ **TBLPROPERTIES ("skip.header.line.count"="1");** indique que nous ne voulons par charger le « header » du fichier source :
```sql
CREATE TABLE IF NOT EXISTS customers (
    id_cust INT,
    name STRING,
    country STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
TBLPROPERTIES ("skip.header.line.count"="1");
```

Maintenant, nous chargerons le fichier dans la table :
```sql
LOAD DATA INPATH '/user/hive/warehouse/customers.csv' INTO TABLE customers;
```
Vérifiez le contenu de la table :
```sql
SELECT * FROM customers ;
```
Nous allons maintenant créer des partitions de cette table de façon à avoir des fragments de la base suivant un certain critère sous le dossier /user/hive/warehouse comme indiqué dans la figure :
![image](https://github.com/user-attachments/assets/df77310c-fd40-4655-9451-baeacf930b31)

Pour ce faire, préparez cette opération de partition, en créant tout d’abord la table **customers_by_country** :
```sql
CREATE TABLE customers_by_country
(id_cust INT, name STRING)
PARTITIONED BY (country string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';
```
Tapez ensuite pour demander à hive d’opter pour la partition dynamique : 
```sql
set hive.exec.dynamic.partition.mode=nonstrict;
```
Puis:
```sql
INSERT OVERWRITE TABLE customers_by_country
PARTITION(country)
SELECT id_cust ,name , country FROM customers;
```
Affichez ces partitions à partir de CLI:
```SQL
SHOW PARTITIONS customers_by_country;
```

Puis, vous sortez de hive cli avec quit; et puis lancez la commande :
```sql
hdfs dfs -ls /user/hive/warehouse/customers_by_country
```

## Bucketing
La technique de bucketing dans Hive permet de répartir les données d’une table en plusieurs fichiers appelés “buckets”.
Chaque ligne est placée dans un bucket en fonction du hash d’une colonne choisie par l’utilisateur, tout en conservant toutes les colonnes de la ligne.
Cela permet d’optimiser les jointures, les agrégations et le sampling sur les grandes tables.

Le bucketing est recommandé pour faire l'opération Map-side join. C'est quoi un Map-side join? Dans un join classique Hive/MapReduce, les mappers lisent les deux tables
Les données sont shufflées vers les reducers selon la clé de jointure et les reducers font le join
→ Coût élevé si les tables sont grandes

Avec un Map-Side Join, Hive charge entièrement la petite table dans la mémoire du mapper, puis chaque mapper lit une portion de la grande table et fait la jointure directement en mémoire
→ Pas de shuffle, pas de phase Reduce → très rapide

Ici, nous allons bucketer la table `customers` par `id_cust` en 8 buckets.
```sql
CREATE TABLE customers_bucketed (
    id_cust INT,
    name STRING,
    country STRING
)
CLUSTERED BY (id_cust) INTO 8 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';
```
Expliquons ces deux lignes:
- `CLUSTERED BY (id_cust)` : on utilise `id_cust` pour répartir les lignes dans les buckets.
- `INTO 8 BUCKETS` : 8 fichiers seront créés dans HDFS, un par bucket.

Pour que Hive applique réellement le bucketing lors de l’insertion, il faut activer :
```sql
SET hive.enforce.bucketing = true;
```
Maintenant, chargez les données dans la table qui a subit le bucketing
```sql
INSERT INTO TABLE customers_bucketed
SELECT * FROM customers;
```

Ainsi, nous pouvons extraire des données des échantillons d'une manière très rapide sans tout charger:
```sql
SELECT * FROM customers_bucketed TABLESAMPLE(BUCKET 3 OUT OF 8);
```
Vous pouvez consulter les fichiers buckets sur le disque physique:
```sql
hdfs dfs -ls /user/hive/warehouse/customers_bucketed
```
## Optimisation avec Hive : CBO et PPD
L’optimisation dans Hive s’appuie fortement sur deux mécanismes essentiels :
- CBO (Cost-Based Optimizer) : l’optimiseur basé sur le coût.
- PPD (Predicate Pushdown) : l’optimisation qui pousse les filtres vers le stockage.

Mais pour tirer pleinement parti de ces optimisations, il est recommandé d’utiliser un format de fichier optimisé, comme ORC.
ORC (Optimized Row Columnar) est un format colonnaire, conçu spécialement pour Hive.
Créeons le fichier orc:
```sql
CREATE TABLE IF NOT EXISTS customers_orc
STORED AS ORC AS
SELECT * FROM customers;
```
Actiovons le CBO;
```sql
SET hive.cbo.enable=true;
SET hive.compute.query.using.stats=true;

ANALYZE TABLE customers_orc COMPUTE STATISTICS FOR COLUMNS;
```
Le Cost-Based Optimizer permet à Hive de choisir le meilleur plan d’exécution en se basant sur :
- le nombre de lignes
- la taille des colonnes
- la distribution des données
- les cardinalités
- les statistiques ORC

Grâce au CBO, Hive peut automatiquement :
- choisir un Map Join plutôt qu’un Reduce Join
- réordonner les tables dans un join
- éviter des lectures inutiles

Passons maintenant au PPD. Le Predicate Pushdown (PPD) consiste à pousser le filtre le plus bas possible, c’est-à-dire directement au niveau du fichier ORC, avant même que Hive ne charge les données en mémoire.
```sql
EXPLAIN
SELECT * FROM customers_orc
WHERE country = 'France';
```

# Manipulation : Pipeline Batch Processing Data

Le but de cette manipulation est la simulation d'extraction des données d'une base de données relationnelles vers hive afin de le traiter ensuite à l'aide de hadoop MapReduce comme est indiqué dans la figure suivante:
<img width="820" height="194" alt="image" src="https://github.com/user-attachments/assets/2e8fcd3b-1255-41a7-bdaa-8546afa6a5ca" />

1. Pour ce faire,1.	Connectez-vous au conteneur hive-server:
```cmd
   docker exec -it hive-server bash
```
2.	Une fois connecté, nous allons lancer un script pour configurer **sqoop**.
```cmd
chmod 7 start-hive.sh
```
Puis lancez : (si vous le lancez pour la première fois, vous aurez un message : rm: `/user/sqoop': No such file or directory)
```cmd
./start-hive.sh
```
La table à créer s'appelle **order_detail** et on va vous guider pour faire ça.
3.	Créer avec hive cli la base de données **lab2_big_data**.
``cmd
hive
```
Puis:
```cmd
CREATE DATABASE lab2_big_data;
```
Quittez hive et le container `hive-server`
4.	Accédez au container de postgres avec la commande :
```cmd
sudo docker exec -it postgres-db psql -U postgres
```
5.	Afficher toutes les bases de données que vous avez à bord avec la commande :
```cmd
\l
```
6.	Vous allez accéder à la base « temp_db » avec la commande :
```cmd 
	\c temp_db
```
7.	Vous pouvez vérifier s’il y a des tables dans la base : 
```cmd 
\dt
```
8.	Nous allons créer une table qui s’appelle order_detail que nous chargerons d’un fichier csv :
```sql
DROP TABLE IF EXISTS order_detail;
```
Ensuite:
```sql
CREATE TABLE IF NOT EXISTS order_detail (
order_created_timestamp TIMESTAMP,
    status VARCHAR,
    price INT,
    discount FLOAT,
    id VARCHAR PRIMARY KEY NOT NULL,
    driver_id VARCHAR,
    user_id VARCHAR,
    restaurant_id VARCHAR
);
```
Puis, lancez cette commande juste après, sous la base temp_db et étant connecté sur la base postgres.
```cmd
COPY order_detail FROM '/var/lib/postgresql/data/lmwn/order_detail.csv' DELIMITER ',' CSV HEADER;
```
Vous venez avec cette commande de charger les données dans le fichier .csv dans la table de base de données sous postgres container. Quittez votre container en tapant:
```cmd
\q
```
11.	Nous allons maintenant importer les données de la table que nous venons de créer sur notre RDBMS vers Hive à l’aide de Sqoop. Tapez la commande suivante :
```cmd
docker exec -it hive-server bash
```
puis à l'intérieur du container, vous lancez:
```cmd
/usr/lib/sqoop/bin/sqoop import --connect jdbc:postgresql://database:5432/temp_db --table order_detail --username postgres --password passw0rd --hive-import --hive-table lab2_big_data.order_detail_hive --num-mappers 1 --map-column-hive order_created_timestamp=STRING,status=STRING,price=INT,discount=FLOAT,id=STRING,driver_id=STRING,user_id=STRING,restaurant_id=STRING
```
Accédez à « localhost :8088 » pour voir le succès de votre transfert de données avec sqoop. La raison pour laquelle vous trouvez ce transfert sur le history server de hadoop est le fait que Hive a déclenché un job MapReduce pour faire cette opération !
![image](https://github.com/user-attachments/assets/621ffe7b-3d22-4841-86d4-43b972555f61)


Vérifier le succès du transfert en revenant vers hive:
```cmd
SELECT * FROM lab2_big_data.order_detail_hive LIMIT 5;
```
## A vous de jouer!
Nous voulons développer un programme MapReduce (en Java) qui calcule le nombre total de commandes par restaurant à partir des fichiers sources suivants :

- `restaurant_detail.csv` : à télécharger du lien drive précédemment mentionné
- `order_detail.csv`

Avant d’écrire le programme MapReduce,
vous devez proposer et mettre en place une architecture de données optimisée dans Hive, afin que les traitements MapReduce puissent exploiter des données compactes, partitionnées et/ou bucketées.

